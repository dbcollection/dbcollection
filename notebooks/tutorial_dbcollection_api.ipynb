{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dbcollection package usage tutorial\n",
    "\n",
    "This tutorial shows how to use the `dbcollection` package to load and manage datasets in a simple and easy way. It is divided into two main topics: \n",
    "<ol>\n",
    "<li>Dataset managing.</li>\n",
    "<li>Fetch data from a dataset.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dbcollection package information\n",
    "\n",
    "Below a brief description of the package and its core APIs is presented so you as a user can immediately start using this package after this tutorial.\n",
    "\n",
    "### Overview \n",
    "This package contains a collection of datasets with pre-defined methods for data download (when available/possible)\n",
    "and processing. A dataset's information is stored into a HDF5 file where the necessary metadata is stored into several groups that (usually) correspond to train, val and/or sets. In turn, the information of which dataset's have been defined in the system is stored in a `.json` cache file in a directory your home directory: `dbcollection/db_cache_info.json`.\n",
    "\n",
    "### Data access\n",
    "To access this file, a special API is available for easy access to data through a few commands. Also, the user can directly access the metadata file if desired. The metadata file contains all information available for each dataset like image file names, class labels, bounding boxes, etc. The metadata is stored into two ways: 1) the original metadata format style is kept and stored under the group '**set**/source/'; and 2) all fields are stored separately into single arrays for fast access and combined by a field named 'object_id' which contains all the indexes relating each field with each other, and all fields are stored under the group '**set**/default/'.\n",
    "\n",
    "### Features\n",
    "\n",
    "The benefits of using this framework allows the following:\n",
    "- A dataset can be setup once and reused as many times as needed\n",
    "- Since data is stored and accessed from disk, the memory footprint is small for any dataset\n",
    "- It has cross-platform (Windows, Linux, MacOS) and cross-language (C/C++, Python, Lua, Matlab) capabilities\n",
    "- Any dataset can be setup/stored using this framework (images, text, etc.)\n",
    "\n",
    "\n",
    "### Dataset managing API\n",
    "\n",
    "For loading/removing/setup datasets, the `dbcollection` package contains module `manager` which has the following methods:\n",
    "- **dbcollection.manager.load(name, task, verbose):** Returns a metadata loader of a dataset.\n",
    "- **dbcollection.manager.setup(name, data_dir, task_name, is_download, verbose):** Setup a dataset's metadata and cache files on disk.\n",
    "- **dbcollection.manager.remove(name, delete_data):** Delete a dataset from the cache.\n",
    "- **dbcollection.manager.manage_cache(field, value, delete_cache, clear_cache, verbose):** Manages the cache file.\n",
    "- **dbcollection.manager.query(pattern):** Do simple queries to the cache.\n",
    "- **dbcollection.manager.info(verbose):** Prints the cache file contents.\n",
    "\n",
    "\n",
    "### Data loading API\n",
    "\n",
    "When loading a dataset, `dbcollection.manager.load()` returns a `class DatasetLoader` which contains some information about the dataset like `name`, `task`, `data_dir`, `cache_path` and, for each set of the dataset (train/val/test/etc), a `ManagerHDF5` class which contains the data fetching methods for the HDF5 metadata file is assigned.\n",
    "\n",
    "The data loader API contains the following methods for data retrieval:\n",
    "- **get(field_name, idx):** Retrieve the i'th data from the field 'field_name'.\n",
    "- **object(idx, is_value):** Retrieves the data's ids or contents of all fields of an object.\n",
    "- **size(field_name):** Returns the number of the elements of a 'field_name'.\n",
    "- **list():** Lists all fields.\n",
    "- **object_field_id(field_name):** Retrieves the index position of a field in the object id list.\n",
    "\n",
    "\n",
    "### Note\n",
    "\n",
    "This package uses a cache file to store all the information about the dataset's data directory, name, available tasks, etc. This file won't be available until you use the dataset managing API methods for the first time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: Setup a dataset \n",
    "\n",
    "This tutorial shows how you can easily setup and load a dataset from the available list with a few simple commands.\n",
    "\n",
    "Here, we will download the MNIST dataset, setup its metadata file with the train+test sets and display the Loader contents for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import tutorial packages\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import dbcollection.manager as dbclt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Display the cache file contents to the screen \n",
    "\n",
    "First, lets check if the MNIST dataset exists on cache. \n",
    "\n",
    "NOTE: if this is the first time using the package, a folder `dbcollection/` will be created in your home directory along with the `db_cache_info.json` cache file inside which contains all of the dataset's information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# display cache files contents\n",
    "dbclt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# display available datasets for download/process in the package\n",
    "dbclt.info(list_datasets=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lets make a directory to store the data\n",
    "path = os.path.join(os.path.expanduser(\"~\"), 'tmp', 'data')\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "# download + setup the MNIST dataset\n",
    "dbclt.setup(name='mnist', data_dir=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# display cache files contents (mnist should be listed now)\n",
    "dbclt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the MNIST dataset\n",
    "loader = dbclt.load(name='mnist')\n",
    "\n",
    "# print the dataset's information\n",
    "print('######### info #########')\n",
    "print('')\n",
    "print('Dataset: ' + loader.name)\n",
    "print('Task: ' + loader.task)\n",
    "print('Data path: ' + loader.data_dir)\n",
    "print('Metadata cache path: ' + loader.cache_path)\n",
    "print('Sets: ', loader.sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Remove a dataset\n",
    "\n",
    "Removing datasets from the list is very simple by calling the **remove()** method.\n",
    "\n",
    "Lets install another dataset, show that it was successfully installed and then remove it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# download + setup the CIFAR10 dataset\n",
    "dbclt.setup(name='cifar10', data_dir=path) # store the dataset's files to ~/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# display cache files contents (cifar10 should be listed now)\n",
    "dbclt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the cifar10 dataset\n",
    "loader = dbclt.load(name='cifar10')\n",
    "\n",
    "# print the dataset's information\n",
    "print('######### info #########')\n",
    "print('')\n",
    "print('Dataset: ' + loader.name)\n",
    "print('Task: ' + loader.task)\n",
    "print('Data path: ' + loader.data_dir)\n",
    "print('Metadata cache path: ' + loader.cache_path)\n",
    "print('Sets: ', loader.sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove the dataset from the cache and delete its files from disk\n",
    "dbclt.remove(name='cifar10', delete_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# display cache files contents (cifar10 shouldn't be listed)\n",
    "dbclt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# to show that the dataset it was removed, lets attempt to load cifar10 again (should give an error)\n",
    "loader = dbclt.load(name='cifar10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Change some information of the cache file\n",
    "\n",
    "In cases where you need to change some information regarding the cache file, you can use the **manage_cache()** method for this. **Note:** This can also be done by modifying the `.json` file directly. \n",
    "\n",
    "Here we'll rename the path of the MNIST data directory to another name and we'll update the cache file information with the new path. Later we'll see the effects of this change when loading data samples from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fetch MNIST data folder path. For this, we'll use the query() method to retrieve \n",
    "# the information about the mnist dataset as a list\n",
    "result = dbclt.query('mnist')\n",
    "\n",
    "print('Result query: ', result)\n",
    "\n",
    "if 'data_dir' in result['mnist']:\n",
    "    data = result['mnist']\n",
    "    data_dir = result['mnist']['data_dir']\n",
    "\n",
    "print('\\nMNIST data directory: {}'.format(data_dir))\n",
    "\n",
    "# rename the directory\n",
    "data_dir_new = data_dir + 'new'\n",
    "os.rename(data_dir, data_dir_new)\n",
    "print('\\nRenamed mnist folder to: {}'.format(data_dir_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# update the path of the data dir in the cache file\n",
    "new_data = data\n",
    "new_data['data_dir'] = data_dir_new\n",
    "dbclt.manage_cache(field='mnist', value=new_data)\n",
    "\n",
    "print('New data: ', new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check if the data directory path was modified\n",
    "dbclt.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Fetch data from a dataset.\n",
    "\n",
    "This tutorial shows how to fetch data from a dataset using an API.\n",
    "\n",
    "The **load()** method returns a `DatasetLoader` class which contains information about the dataset like the name, task, data paths, sets and a handler for the metadata (HDF5) file for direct access. Also, for each set, a `ManagerHDF5` class is setup so you can easily access data with a simple API to retrieve data from the metadata file.\n",
    "\n",
    "In this tutorial we will use the `MNIST` dataset to retrieve and display data by using the API to fetch data and by directly accessing the data by using the HDF5 file handler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOTE: in order to store strings in the HDF5 file, they are converted to ascii format \n",
    "# and then stored as a numpy array of type 'numpy.uint8'. So, when retrieving string \n",
    "# fields, we must convert them from ascii back to str. This package contains some utility\n",
    "# functions for this task.\n",
    "from dbcollection.utils import convert_ascii_to_str as _tostr\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load the MNIST dataset\n",
    "\n",
    "Load the `MNIST` dataset using the `default` task.\n",
    "\n",
    "<blockquote>\n",
    "<p>NOTE: many datasets like [MSCOCO](http://mscoco.org) have multiple tasks like **object detection**, **caption** or **human body joint keypoint detection**. In order to cope with this, we store each task into a separate HDF5 file by name. Then, when loading a dataset, one just needs to specify which dataset and task to load.</p>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load mnist\n",
    "mnist_loader = dbclt.load(name='mnist', task='default')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: List all data fields composing the MNIST metadata\n",
    "\n",
    "Usually, different datasets have different attributes/data fields/annotations. The **list()** method returns all data fields of a dataset.\n",
    "\n",
    "Here we'll use the `train` set to fetch this information, but you could retrieve this information from the `test` set as well. For the rest of the steps we'll continue to use the `train` set as the source of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fetch data fields\n",
    "fields = mnist_loader.train.list()\n",
    "\n",
    "print('MNIST fields:')\n",
    "print(fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Fetch all class labels and print them\n",
    "\n",
    "Fetch the class names/labels of the mnist dataset using the **get()** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fetch class labels\n",
    "labels = mnist_loader.train.get('classes')\n",
    "\n",
    "print('MNIST class labels:')\n",
    "print(_tostr(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Show data size\n",
    "\n",
    "To get the size of any field you need to use the **size()** method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# show size of the images data\n",
    "print('Total images:', mnist_loader.train.size('data'))\n",
    "print('Image data size:', mnist_loader.train.size('data', True)) # return the shape of the array\n",
    "print('')\n",
    "\n",
    "# show the size of the labels\n",
    "print('Total labels:', mnist_loader.train.size('labels'))\n",
    "print('Label size:', mnist_loader.train.size('labels', True)) # return the shape of the array\n",
    "print('')\n",
    "\n",
    "# show the size of the object_id list\n",
    "print('Total objects:', mnist_loader.train.size('labels'))\n",
    "print('Objects size:', mnist_loader.train.size('labels', True)) # return the shape of the array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Fetch an image + label\n",
    "\n",
    "The image's data + label are grouped together in a field named `object_id` (this is true for any dataset). This is usefull in some cases and not in others. In the `MNIST` case, having only the image data and labels information would suffice, but in other cases it would be impossible to keep track of what matches with.\n",
    "\n",
    "For example, in object detection tasks likes MSCOCO or the Pascal VOC, images usually contain several objects and each has its own class label. The easiest way to store such relationships between images and objects is to use a list of indexes of each data field like filename, label, bounding box, etc.\n",
    "\n",
    "Here, the `object_id` field contains the indexes of both images and labels. To fetch this information, you have two choices:\n",
    "<ol>\n",
    "<li>Use **get('object_id', idx)**</li>\n",
    "<li>Use **object(idx)** to fetch a list of indexes.</li>\n",
    "</ol>\n",
    "\n",
    "Although both returns the same output, the **object()** can return either a list of indexes or a list of values, i.e., it automatically fetch the data of all fields w.r.t their indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fetch the first image + label\n",
    "# fetch data using get()\n",
    "list_idx = mnist_loader.train.get('object_ids', 0)\n",
    "img = mnist_loader.train.get('data', list_idx[0])\n",
    "label = mnist_loader.train.get('labels', list_idx[1])\n",
    "\n",
    "# fetch the same data using object()\n",
    "img2, label2 = mnist_loader.train.object(0, True) #True - return values | False - return indexes\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2)\n",
    "axs[0].set_title('Method 1 (get): label {}'.format(label))\n",
    "axs[0].imshow(img, cmap=plt.cm.gray)\n",
    "axs[1].set_title('Method 2 (object): label {}'.format(label2))\n",
    "axs[1].imshow(img2 cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Fetch random images and display them\n",
    "\n",
    "This example loads images randomly and displays them in a 8x8 grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "# get data size\n",
    "img_size = mnist_loader.train.size('data')\n",
    "\n",
    "fig = plt.figure(1, (6., 6.))\n",
    "grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                 nrows_ncols=(8, 8),  # creates 8*8 grid of axes\n",
    "                 axes_pad=0.05,  # pad between axes in inch.\n",
    "                 )\n",
    "\n",
    "for i in range(8*8):\n",
    "    img, label = mnist_loader.train.object(random.randint(1, img_size), True)\n",
    "    grid[i].axis('off')\n",
    "    grid[i].imshow(img, cmap=plt.cm.gray)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Access data through python's HDF5 file API \n",
    "\n",
    "You can directly access the metadata's data file from the data loader. This allows the user to access the data in two formats:\n",
    "<ol>\n",
    "<li>`default` format, where each data field is stored as a single and separate numpy array and where the fields are combined by the `object_id` field.</li>\n",
    "<li>`source` format, where the data is stored in the dataset's original format (usually in a nested scheme).</li>\n",
    "</ol>\n",
    "\n",
    "These two formats are translated to two distinct groups in the HDF5 file for each set (train/val/test/etc). They are defined by `default/` (API friendly) and `source/` (original data format). Since some users might prefer one approach over the other, by given both choices it should provide the most adequate option for most users and/or situations.\n",
    "\n",
    "In the following example, data is retrieved by directly using the python's HDF5 API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get the HDF5 file handler\n",
    "mnist_h5 = mnist_loader.file.storage\n",
    "\n",
    "# fetch a random image and label from the test set\n",
    "size = len(mnist_h5['test/source/labels'])\n",
    "idx = random.randint(1, size)\n",
    "img = mnist_h5['test/source/data'][idx]\n",
    "label = mnist_h5['test/source/labels'][idx]\n",
    "\n",
    "print('Display image nÂº{}'.format(idx))\n",
    "print('Label: {}'.format(label))\n",
    "plt.imshow(img, cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
